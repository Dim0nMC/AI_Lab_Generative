{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def convert_html_to_text(html_file):\n",
    "    with open(html_file, 'rb') as file:\n",
    "        raw_content = file.read()\n",
    "        detection = chardet.detect(raw_content)\n",
    "        file_encoding = detection['encoding']\n",
    "    \n",
    "    with open(html_file, 'r', encoding=file_encoding) as file:\n",
    "        parser = BeautifulSoup(file, 'html.parser')\n",
    "        text_content = parser.get_text()\n",
    "    return text_content\n",
    "\n",
    "\n",
    "html_file_path = 'D:\\\\Study\\\\3grade\\\\AI\\\\Labs3\\\\Гарри.html'\n",
    "book_text = convert_html_to_text(html_file_path)\n",
    "\n",
    "\n",
    "with open('bookG.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(book_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import Sequence, to_categorical\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, Embedding, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, Callback\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина текста: 3598251 символов\n"
     ]
    }
   ],
   "source": [
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "text_file_path = 'D:\\\\Study\\\\3grade\\\\AI\\\\Labs3\\\\bookG.txt'\n",
    "book_text = read_text_file(text_file_path)\n",
    "\n",
    "print(f\"Длина текста: {len(book_text)} символов\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Подготовка данных для посимвольной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество последовательностей: 1199404\n",
      "Форма X_char: (1199404, 40, 171), Форма y_char: (1199404, 171)\n"
     ]
    }
   ],
   "source": [
    "unique_chars = sorted(list(set(book_text)))\n",
    "char_to_index = {char: unique_chars.index(char) for char in unique_chars}\n",
    "index_to_char = {index: char for index, char in enumerate(unique_chars)}\n",
    "\n",
    "sequence_length = 40  \n",
    "step_size = 3\n",
    "\n",
    "text_sequences = []\n",
    "next_characters = []\n",
    "for i in range(0, len(book_text) - sequence_length, step_size):\n",
    "    text_sequences.append(book_text[i: i + sequence_length])\n",
    "    next_characters.append(book_text[i + sequence_length])\n",
    "\n",
    "print(f'Количество последовательностей: {len(text_sequences)}')\n",
    "\n",
    "X_char_data = np.zeros((len(text_sequences), sequence_length, len(unique_chars)), dtype=bool)\n",
    "y_char_data = np.zeros((len(text_sequences), len(unique_chars)), dtype=bool)\n",
    "for i, sequence in enumerate(text_sequences):\n",
    "    for t, char in enumerate(sequence):\n",
    "        X_char_data[i, t, char_to_index[char]] = 1\n",
    "    y_char_data[i, char_to_index[next_characters[i]]] = 1\n",
    "\n",
    "print(f\"Форма X_char: {X_char_data.shape}, Форма y_char: {y_char_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных для модели на уровне слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество уникальных слов: 10000\n",
      "Форма X_word: (496817, 40), Форма y_word: (496817,)\n"
     ]
    }
   ],
   "source": [
    "max_words = 10000\n",
    "text_tokenizer = Tokenizer(num_words=max_words)\n",
    "text_tokenizer.fit_on_texts([book_text])\n",
    "\n",
    "word_sequences = text_tokenizer.texts_to_sequences([book_text])[0]\n",
    "word_index = text_tokenizer.word_index\n",
    "\n",
    "print(f\"Количество уникальных слов: {min(len(word_index), max_words)}\")\n",
    "\n",
    "X_word_data = []\n",
    "y_word_data = []\n",
    "for i in range(sequence_length, len(word_sequences)):\n",
    "    X_word_data.append(word_sequences[i-sequence_length:i])\n",
    "    y_word_data.append(word_sequences[i])\n",
    "\n",
    "X_word_data = np.array(X_word_data)\n",
    "y_word_data = np.array(y_word_data)\n",
    "\n",
    "print(f\"Форма X_word: {X_word_data.shape}, Форма y_word: {y_word_data.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генератор данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceDataGenerator(Sequence):\n",
    "    def __init__(self, sequences, labels, batch_size, vocab_size, seq_length, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.sequences) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_sequences = self.sequences[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "\n",
    "        X = np.zeros((len(batch_sequences), self.seq_length))\n",
    "        y = np.zeros((len(batch_sequences), self.vocab_size + 1))\n",
    "\n",
    "        for i, sequence in enumerate(batch_sequences):\n",
    "            X[i] = sequence\n",
    "            y[i] = to_categorical(batch_labels[i], num_classes=self.vocab_size + 1)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "batch_size = 128\n",
    "data_generator = SequenceDataGenerator(X_word_data, y_word_data, batch_size, max_words, sequence_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение посимвольной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m9371/9371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 35ms/step - loss: 2.5490 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m9371/9371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m285s\u001b[0m 30ms/step - loss: 2.0940 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m9371/9371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m288s\u001b[0m 31ms/step - loss: 2.0304 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m9371/9371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 31ms/step - loss: 2.0072 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m9371/9371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m287s\u001b[0m 31ms/step - loss: 1.9950 - learning_rate: 0.0010\n",
      "Epoch 6/20\n",
      "\u001b[1m9371/9371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 28ms/step - loss: 1.9871 - learning_rate: 0.0010\n",
      "Epoch 7/20\n",
      "\u001b[1m9371/9371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m266s\u001b[0m 28ms/step - loss: 1.9847 - learning_rate: 0.0010\n",
      "Epoch 8/20\n",
      "\u001b[1m9371/9371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 28ms/step - loss: 1.9824 - learning_rate: 0.0010\n",
      "Epoch 9/20\n",
      "\u001b[1m9371/9371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 28ms/step - loss: 1.9796 - learning_rate: 0.0010\n",
      "Epoch 10/20\n",
      "\u001b[1m9371/9371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 28ms/step - loss: 1.9812 - learning_rate: 0.0010\n",
      "Epoch 11/20\n",
      "\u001b[1m9371/9371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 28ms/step - loss: 1.9288 - learning_rate: 2.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m9371/9371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 28ms/step - loss: 1.9102 - learning_rate: 2.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m9371/9371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 28ms/step - loss: 1.9040 - learning_rate: 2.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m9371/9371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 29ms/step - loss: 1.8992 - learning_rate: 2.0000e-04\n",
      "Epoch 15/20\n",
      "\u001b[1m9371/9371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 29ms/step - loss: 1.8972 - learning_rate: 2.0000e-04\n",
      "Epoch 16/20\n",
      "\u001b[1m9371/9371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 29ms/step - loss: 1.8914 - learning_rate: 2.0000e-04\n",
      "Epoch 17/20\n",
      "\u001b[1m9371/9371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 29ms/step - loss: 1.8898 - learning_rate: 2.0000e-04\n",
      "Epoch 18/20\n",
      "\u001b[1m9371/9371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 29ms/step - loss: 1.8874 - learning_rate: 2.0000e-04\n",
      "Epoch 19/20\n",
      "\u001b[1m9371/9371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 29ms/step - loss: 1.8840 - learning_rate: 2.0000e-04\n",
      "Epoch 20/20\n",
      "\u001b[1m9371/9371\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 29ms/step - loss: 1.8849 - learning_rate: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "char_rnn_model = Sequential()\n",
    "char_rnn_model.add(Input(shape=(sequence_length, len(unique_chars))))\n",
    "char_rnn_model.add(SimpleRNN(128, return_sequences=True))\n",
    "char_rnn_model.add(Dropout(0.2))\n",
    "char_rnn_model.add(SimpleRNN(128))\n",
    "char_rnn_model.add(Dropout(0.2))\n",
    "char_rnn_model.add(Dense(len(unique_chars), activation='softmax'))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "char_rnn_model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint('char_rnn_best.keras', save_best_only=True, monitor='loss', mode='min')\n",
    "lr_reduction_callback = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "\n",
    "history = char_rnn_model.fit(X_char_data, y_char_data, batch_size=128, epochs=20, verbose=1, callbacks=[checkpoint_callback, lr_reduction_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение модели на уровне слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 50ms/step - loss: 7.1305 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 50ms/step - loss: 6.3017 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 50ms/step - loss: 5.8772 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 50ms/step - loss: 5.6131 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 51ms/step - loss: 5.4372 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 51ms/step - loss: 5.2984 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 51ms/step - loss: 5.1881 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 51ms/step - loss: 5.0955 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 51ms/step - loss: 5.0163 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 51ms/step - loss: 4.9538 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 51ms/step - loss: 4.8850 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 51ms/step - loss: 4.8332 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 51ms/step - loss: 4.7786 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 52ms/step - loss: 4.7448 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 51ms/step - loss: 4.7065 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 52ms/step - loss: 4.6567 - learning_rate: 0.0010\n",
      "Epoch 17/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 52ms/step - loss: 4.6254 - learning_rate: 0.0010\n",
      "Epoch 18/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 52ms/step - loss: 4.5911 - learning_rate: 0.0010\n",
      "Epoch 19/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 52ms/step - loss: 4.5523 - learning_rate: 0.0010\n",
      "Epoch 20/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 52ms/step - loss: 4.5334 - learning_rate: 0.0010\n",
      "Epoch 21/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 52ms/step - loss: 4.5000 - learning_rate: 0.0010\n",
      "Epoch 22/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 52ms/step - loss: 4.4758 - learning_rate: 0.0010\n",
      "Epoch 23/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 52ms/step - loss: 4.4590 - learning_rate: 0.0010\n",
      "Epoch 24/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 52ms/step - loss: 4.4289 - learning_rate: 0.0010\n",
      "Epoch 25/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 54ms/step - loss: 4.4131 - learning_rate: 0.0010\n",
      "Epoch 26/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 52ms/step - loss: 4.3947 - learning_rate: 0.0010\n",
      "Epoch 27/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 52ms/step - loss: 4.3820 - learning_rate: 0.0010\n",
      "Epoch 28/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 52ms/step - loss: 4.3551 - learning_rate: 0.0010\n",
      "Epoch 29/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 52ms/step - loss: 4.3403 - learning_rate: 0.0010\n",
      "Epoch 30/30\n",
      "\u001b[1m3882/3882\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m206s\u001b[0m 53ms/step - loss: 4.3274 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "word_rnn_model = Sequential()\n",
    "word_rnn_model.add(Input(shape=(sequence_length,)))\n",
    "word_rnn_model.add(Embedding(input_dim=max_words + 1, output_dim=50))\n",
    "word_rnn_model.add(SimpleRNN(128, return_sequences=True))\n",
    "word_rnn_model.add(Dropout(0.2))\n",
    "word_rnn_model.add(SimpleRNN(128))\n",
    "word_rnn_model.add(Dense(max_words + 1, activation='softmax'))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "word_rnn_model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint('word_rnn_best.keras', save_best_only=True, monitor='loss', mode='min')\n",
    "lr_reduction_callback = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001)\n",
    "\n",
    "history = word_rnn_model.fit(data_generator, epochs=30, verbose=1, callbacks=[checkpoint_callback, lr_reduction_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерации текста посимвольной моделью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сгенерированный текст (посимвольная модель):\n",
      "Однажды...  …   .,  ,    .»,: ?...     .,., . .., . .   ... . ,.   .  . , . .., ,..,,. .  ..,,  .,. ..    .\n"
     ]
    }
   ],
   "source": [
    "def predict_next_char(predictions, temperature=1.0):\n",
    "    predictions = np.asarray(predictions).astype('float64')\n",
    "    predictions = np.log(predictions + 1e-8) / temperature\n",
    "    exp_predictions = np.exp(predictions)\n",
    "    predictions = exp_predictions / np.sum(exp_predictions)\n",
    "    probability = np.random.multinomial(1, predictions, 1)\n",
    "    return np.argmax(probability)\n",
    "\n",
    "def generate_text_with_char_rnn(model, initial_text, char_to_index, index_to_char, seq_length, num_chars, temperature=1.0):\n",
    "    generated_text = initial_text\n",
    "    current_sequence = initial_text[-seq_length:]\n",
    "\n",
    "    for _ in range(num_chars):\n",
    "        x_input = np.zeros((1, seq_length, len(char_to_index)))\n",
    "        for t, char in enumerate(current_sequence):\n",
    "            if char in char_to_index:\n",
    "                x_input[0, t, char_to_index[char]] = 1.0\n",
    "\n",
    "        prediction_probs = model.predict(x_input, verbose=0)[0]\n",
    "        next_char_index = predict_next_char(prediction_probs, temperature)\n",
    "        next_char = index_to_char[next_char_index]\n",
    "\n",
    "        generated_text += next_char\n",
    "        current_sequence = current_sequence[1:] + next_char\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "initial_text_char = \"Однажды\"\n",
    "generated_text_char = generate_text_with_char_rnn(char_rnn_model, initial_text_char, char_to_index, index_to_char, seq_length=60, num_chars=100, temperature=0.5)\n",
    "print(f\"Сгенерированный текст (посимвольная модель):\\n{generated_text_char}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерации текста пословной моделью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сгенерированный текст (модель на уровне слов):\n",
      "Это начало текста для генерации в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в в не в занятий директора — я не могу рассказать тебе — сказал гарри — я не\n"
     ]
    }
   ],
   "source": [
    "def generate_text_with_word_rnn(model, tokenizer, initial_text, seq_length, num_words):\n",
    "    generated_text = initial_text\n",
    "    current_sequence = tokenizer.texts_to_sequences([initial_text])[0]\n",
    "\n",
    "    for _ in range(num_words):\n",
    "        x_input = np.zeros((1, seq_length))\n",
    "        for t, word_index in enumerate(current_sequence[-seq_length:]):\n",
    "            x_input[0, t] = word_index\n",
    "\n",
    "        prediction_probs = model.predict(x_input, verbose=0)[0]\n",
    "        next_word_index = np.argmax(prediction_probs)\n",
    "        next_word = tokenizer.index_word[next_word_index]\n",
    "\n",
    "        generated_text += \" \" + next_word\n",
    "        current_sequence.append(next_word_index)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "initial_text_word = \"Это начало текста для генерации\"\n",
    "generated_text_word = generate_text_with_word_rnn(word_rnn_model, text_tokenizer, initial_text_word, seq_length=40, num_words=50)\n",
    "print(f\"Сгенерированный текст (модель на уровне слов):\\n{generated_text_word}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
