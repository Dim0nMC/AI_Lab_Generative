# Отчёт по лабораторной работе
## Генеративные текстовые нейросети

### Студенты: 

| ФИО                | Роль в проекте              | Оценка       |
|--------------------|-----------------------------|--------------|
| Селивёрстов Дмитрий| RNN, отчёт                  |              |
| Меджидли Исмаил    | LSTM, отчёт                 |              |
| Дубровин Дмитрий   | Двунаправленная LSTM, отчёт |              |

## RNN

Для обучения RNN была выбрана книга «Гарри Поттер», которая была конвертирована из HTML-файла в текст. Для модели были использованы два типа токенизации: на уровне символов и на уровне слов. Модель прошла 20 эпох для посимвольной токенизации и 30 эпох для токенизации на уровне слов. Каждое последующее слово/символ выбиралось случайно с учетом вероятностей, предсказанных моделью.

### Посимвольная модель
Модель обучалась на последовательностях длиной 40 символов с шагом 3. Для обучения использовались две слои RNN по 128 нейронов с добавлением слоев Dropout для предотвращения переобучения. Пример сгенерированного текста из 100 символов на основе строки "Однажды":
```
Однажды...  …   .,  ,    .»,: ?...     .,., . .., . .   ... . ,.   .  . , . .., ,..,,. .  ..,,  .,. ..    .
```
Сгенерированный текст практически не имеет смысла, что связано с тем, что модель обучена на уровне символов. Она может предсказать отдельные символы, но не всегда формирует осмысленные слова или предложения.

### Модель на уровне слов
Для модели с токенизацией по словам было использовано до 10 000 уникальных слов, каждая последовательность имела длину 40 слов. Пример сгенерированного текста на основе фразы "Это начало текста для генерации":
```
Это начало текста для генерации в в в в в в в в в в в в в в в в в в в в в в в в в в не в занятий директора — я не могу рассказать тебе — сказал гарри — я не
```
Текст имеет больше осмысленных слов, но его структура страдает от повторений, что может быть связано с ограниченным количеством данных для обучения.

Модели RNN продемонстрировали возможность генерации текста на основе как символов, так и слов. Однако качество генерируемого текста пока остается на низком уровне, особенно для посимвольной модели. Из-за ограничений вычислительных ресурсов и объема данных обучение на большом корпусе текстов представляется затруднительным. Например, обучение посимвольной модели на протяжении 20 эпох заняло 1 час, а обучение пословной модели — около 3 часов.

## LSTM

Для обучения LSTM была выбрана книга Олега Авраменко “Власть молнии”. Токенизация проводилась на уровне слов и символов для улучшения качества создаваемого текста. Модель прошла 20 обучающих эпох. Каждое последующее слово выбиралось на основе вероятностей, предсказанных моделью.

### Однослойная модель LSTM по символам
Модель включает следующие слои:
* Input: Входной слой, принимающий последовательности символов.
* LSTM (128): Слой LSTM с 128 нейронами, который обрабатывает последовательности символов.
* Dense: Полносвязный слой с функцией активации softmax для предсказания следующего символа.

### Многослойная модель LSTM по символам
Модель включает следующие слои:
* Input: Входной слой, принимающий последовательности символов.
* LSTM (128, return_sequences=True): Первый слой LSTM с 128 нейронами, возвращающий последовательности для следующего слоя.
* LSTM (128): Второй слой LSTM с 128 нейронами, который обрабатывает выходные данные первого слоя.
* Dense: Полносвязный слой с функцией активации softmax для предсказания следующего символа.

### Однослойная модель LSTM по словам
Модель включает следующие слои:
* Embedding: Преобразует слова в векторы фиксированной длины (100).
* LSTM (128): Слой LSTM с 128 нейронами, который обрабатывает последовательности слов.
* Dense: Полносвязный слой с функцией активации softmax для предсказания следующего слова.

### Многослойная модель LSTM по словам
Модель включает следующие слои:
* Embedding: Преобразует слова в векторы фиксированной длины (100).
* LSTM (128, return_sequences=True): Первый слой LSTM с 128 нейронами, возвращающий последовательности для следующего слоя.
* LSTM (128): Второй слой LSTM с 128 нейронами, который обрабатывает выходные данные первого слоя.
Dense: Полносвязный слой с функцией активации softmax для предсказания следующего слова.

### Токенизация с использованием BPE (Byte Pair Encoding)
Для токенизации текста использовалась модель BPE с размером словаря 30000. Токенизация проводилась на уровне символов.

### Однослойная модель LSTM с использованием BPE
Модель включает следующие слои:
* Embedding: Преобразует токены в векторы фиксированной длины (100).
* LSTM (128): Слой LSTM с 128 нейронами, который обрабатывает последовательности токенов.
* Dense: Полносвязный слой с функцией активации softmax для предсказания следующего токена.

### Многослойная модель LSTM с использованием BPE
Модель включает следующие слои:
* Embedding: Преобразует токены в векторы фиксированной длины (100).
* LSTM (128, return_sequences=True): Первый слой LSTM с 128 нейронами, возвращающий последовательности для следующего слоя.
* LSTM (128): Второй слой LSTM с 128 нейронами, который обрабатывает выходные данные первого слоя.
* Dense: Полносвязный слой с функцией активации softmax для предсказания следующего токена.

Процесс обучения моделей и примеры сгенерированных текстов находятся в файле "LSTM.ipynb". Ниже текст, сгенерированный многослойной моделью LSTM по слову "Однажды":
```
Однажды спал ненаглядная наверняка подглядывает угрожающей отступила тебе ваша разыграв скрывает медленно главы удивляло начинает траурно изначальную переварил каким песнопение догадывался пожал главный разбил холода хотя бор ти име во кричали мысленно подыскиваешь главный никак на выдворил чтобы строгое нанимаю знакомы употребляют человека безумием сопровождающим запомнит доверие чернявого севе давно поспит некоторое теменем прямо в сундуки с искажены неудачу ему разграбления следивший рукой осмелели фамильными очевидным энергичными лишь огней верно доброжелательно немощного пыток бодрости старику позу мучительной персонаж задохнуться угомонилась мы одними довольно забудет была челюсти когда переполненном сожаление кто осудит огне неплохие пристроил прочих обыкновенных необъяснимая уже нравиться защиту схваткой
```

Модели LSTM продемонстрировали способность генерировать тексты на основе слов, символов и BPE. Однако, полученные тексты имеют низкое качество и недостаток смысла, особенно в случае с посимвольной моделью. Это, вероятно, связано с недостаточным объемом данных для обучения. Тем не менее, из-за ограничений вычислительных ресурсов, обучение модели на больших текстовых корпусах оказывается непрактичным.

## Двунаправленная LSTM

Для обучения двунаправленной ```LSTM``` была выбрана книга  "The champion" от автора **Charles Egbert Craddock**, которая была скачана и сохранена в формате ```txt```. Я использовал токенизацию для преобразования текста в числовой формат (именно для этого была скачана и установлена ```punkt``` от ```nltk```)

Также был разработан класс, который загружает данные частями (**батчами**) вместо всего сразу. Это помогает избежать проблем с перегрузкой оперативной памяти и делает процесс обучения модели на больших объемах данных более эффективным.

Структура **модели**, которую я создал такая:
 - Слой ```Embedding```: преобразует слова в числовые векторы, чтобы модель могла их понять
 - Двунаправленная ```LSTM```: анализирует текст с двух сторон, что в теории помогает ей лучше понять контекст
 - Слой ```Dense```: прогноз следующего слова

После обучения я разработал функцию, которая принимает текст от пользователя, переводит его в числовую последовательность, после чего используется обученная модель для предсказания следующих слов.

**Результат предсказания** при вводе: ```Peter Bateman```
```
Peter Bateman pride turning oblivious reveal reveal accepted quick realized greatly jobs Batemans nowthe message reciprocated message outer uncommunicativeness Whenever arraying week sequence libel suspicious discover Cambridge grip Silence defunct nettling overzealous city lurid No dabbles startled backward memories outright march propelling Refund unmistakably Yes unfamiliar shameless surly imagine designs imagine
```
Качество предсказания конечно оставляет желать лучшего, однако можно заметить, что нет каких то критических ошибок, повторов и т.д. Я предполагаю, что качества осмысленного текста, который может выдать модель, напрямую зависит от размера входных данных для обучения, в следствии чего, для улучшения предсказания нужно использовать более сложные структуры модели или увеличенное количество входных данных.

## Вывод

В результате работы с различными моделями RNN и LSTM, можно сделать вывод, что использование более сложных структур, таких как двунаправленная LSTM, приводит к улучшению качества генерируемого текста, но осмысленности текста так и не получилось. Для ее достижения  требуется больший объем данных и дополнительные улучшения модели.

В целом лабораторная работа помогла нам в изучении новых моделей для обработки и генерации текста, что определенно даст преимущества в будущей работе.
