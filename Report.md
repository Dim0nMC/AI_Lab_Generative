# Отчёт по лабораторной работе
## Генеративные текстовые нейросети

### Студенты: 

| ФИО                | Роль в проекте              | Оценка       |
|--------------------|-----------------------------|--------------|
| Селивёрстов Дмитрий| RNN, отчёт                  |              |
| Меджидли Исмаил    | LSTM, отчёт                 |              |
| Дубровин Дмитрий   | Двунаправленная LSTM, отчёт |              |

## RNN

Для обучения RNN была выбрана книга «Гарри Поттер», которая была конвертирована из HTML-файла в текст. Для модели были использованы два типа токенизации: на уровне символов и на уровне слов. Модель прошла 20 эпох для посимвольной токенизации и 30 эпох для токенизации на уровне слов. Каждое последующее слово/символ выбиралось случайно с учетом вероятностей, предсказанных моделью.

### Посимвольная модель
Модель обучалась на последовательностях длиной 40 символов с шагом 3. Для обучения использовались две слои RNN по 128 нейронов с добавлением слоев Dropout для предотвращения переобучения. Пример сгенерированного текста из 100 символов на основе строки "Однажды":
```
Однажды...  …   .,  ,    .»,: ?...     .,., . .., . .   ... . ,.   .  . , . .., ,..,,. .  ..,,  .,. ..    .
```
Сгенерированный текст практически не имеет смысла, что связано с тем, что модель обучена на уровне символов. Она может предсказать отдельные символы, но не всегда формирует осмысленные слова или предложения.

### Модель на уровне слов
Для модели с токенизацией по словам было использовано до 10 000 уникальных слов, каждая последовательность имела длину 40 слов. Пример сгенерированного текста на основе фразы "Это начало текста для генерации":
```
Это начало текста для генерации в в в в в в в в в в в в в в в в в в в в в в в в в в не в занятий директора — я не могу рассказать тебе — сказал гарри — я не
```
Текст имеет больше осмысленных слов, но его структура страдает от повторений, что может быть связано с ограниченным количеством данных для обучения.

Модели RNN продемонстрировали возможность генерации текста на основе как символов, так и слов. Однако качество генерируемого текста пока остается на низком уровне, особенно для посимвольной модели. Из-за ограничений вычислительных ресурсов и объема данных обучение на большом корпусе текстов представляется затруднительным. Например, обучение посимвольной модели на протяжении 20 эпох заняло 1 час, а обучение пословной модели — около 3 часов.

## LSTM

Для обучения LSTM была выбрана книга Олега Авраменко “Власть молнии”. Токенизация проводилась на уровне слов и символов для улучшения качества создаваемого текста. Модель прошла 20 обучающих эпох. Каждое последующее слово выбиралось на основе вероятностей, предсказанных моделью.

### Однослойная модель LSTM по символам
Модель включает следующие слои:
* Input: Входной слой, принимающий последовательности символов.
* LSTM (128): Слой LSTM с 128 нейронами, который обрабатывает последовательности символов.
* Dense: Полносвязный слой с функцией активации softmax для предсказания следующего символа.

### Многослойная модель LSTM по символам
Модель включает следующие слои:
* Input: Входной слой, принимающий последовательности символов.
* LSTM (128, return_sequences=True): Первый слой LSTM с 128 нейронами, возвращающий последовательности для следующего слоя.
* LSTM (128): Второй слой LSTM с 128 нейронами, который обрабатывает выходные данные первого слоя.
* Dense: Полносвязный слой с функцией активации softmax для предсказания следующего символа.

### Однослойная модель LSTM по словам
Модель включает следующие слои:
* Embedding: Преобразует слова в векторы фиксированной длины (100).
* LSTM (128): Слой LSTM с 128 нейронами, который обрабатывает последовательности слов.
* Dense: Полносвязный слой с функцией активации softmax для предсказания следующего слова.

### Многослойная модель LSTM по словам
Модель включает следующие слои:
* Embedding: Преобразует слова в векторы фиксированной длины (100).
* LSTM (128, return_sequences=True): Первый слой LSTM с 128 нейронами, возвращающий последовательности для следующего слоя.
* LSTM (128): Второй слой LSTM с 128 нейронами, который обрабатывает выходные данные первого слоя.
Dense: Полносвязный слой с функцией активации softmax для предсказания следующего слова.

### Токенизация с использованием BPE (Byte Pair Encoding)
Для токенизации текста использовалась модель BPE с размером словаря 30000. Токенизация проводилась на уровне символов.

### Однослойная модель LSTM с использованием BPE
Модель включает следующие слои:
* Embedding: Преобразует токены в векторы фиксированной длины (100).
* LSTM (128): Слой LSTM с 128 нейронами, который обрабатывает последовательности токенов.
* Dense: Полносвязный слой с функцией активации softmax для предсказания следующего токена.

### Многослойная модель LSTM с использованием BPE
Модель включает следующие слои:
* Embedding: Преобразует токены в векторы фиксированной длины (100).
* LSTM (128, return_sequences=True): Первый слой LSTM с 128 нейронами, возвращающий последовательности для следующего слоя.
* LSTM (128): Второй слой LSTM с 128 нейронами, который обрабатывает выходные данные первого слоя.
* Dense: Полносвязный слой с функцией активации softmax для предсказания следующего токена.

Процесс обучения моделей и примеры сгенерированных текстов находятся в файле "LTSM.ipynb". Ниже текст, сгенерированный многослойной моделью LTSM по слову "Однажды":
```
Однажды спал ненаглядная наверняка подглядывает угрожающей отступила тебе ваша разыграв скрывает медленно главы удивляло начинает траурно изначальную переварил каким песнопение догадывался пожал главный разбил холода хотя бор ти име во кричали мысленно подыскиваешь главный никак на выдворил чтобы строгое нанимаю знакомы употребляют человека безумием сопровождающим запомнит доверие чернявого севе давно поспит некоторое теменем прямо в сундуки с искажены неудачу ему разграбления следивший рукой осмелели фамильными очевидным энергичными лишь огней верно доброжелательно немощного пыток бодрости старику позу мучительной персонаж задохнуться угомонилась мы одними довольно забудет была челюсти когда переполненном сожаление кто осудит огне неплохие пристроил прочих обыкновенных необъяснимая уже нравиться защиту схваткой
```

Модели LSTM продемонстрировали способность генерировать тексты на основе слов, символов и BPE. Однако, полученные тексты имеют низкое качество и недостаток смысла, особенно в случае с посимвольной моделью. Это, вероятно, связано с недостаточным объемом данных для обучения. Тем не менее, из-за ограничений вычислительных ресурсов, обучение модели на больших текстовых корпусах оказывается непрактичным.

## Двунаправленная LSTM

## Вывод
